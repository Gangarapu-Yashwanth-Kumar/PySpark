{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efa0fce",
   "metadata": {},
   "source": [
    "### Pyspark GroupBy And Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f336300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23513a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('GroupBy-Aggregate').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "248b3a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-2936CE4:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>GroupBy-Aggregate</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2844ee93350>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3656e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Filter all warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3bd081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv(\"D:\\APACHE SPARK\\sample3.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed791ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   Name|  Department|Salary|\n",
      "+-------+------------+------+\n",
      "|  Rahul|Data Science| 10000|\n",
      "|   Abhi|          ML|  5000|\n",
      "| Mahesh|          AI|  4000|\n",
      "| Joseph|          AI|  4000|\n",
      "| Shashi|Data Science|  3000|\n",
      "|   Teja|         NLP| 20000|\n",
      "|Akshith|          AI| 10000|\n",
      "|Karthik|          ML|  5000|\n",
      "|   Ajay|Data Science| 10000|\n",
      "|   Ravi|         NLP|  2000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57d24ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f15f8197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   Name|sum(Salary)|\n",
      "+-------+-----------+\n",
      "|Karthik|       5000|\n",
      "|   Ravi|       2000|\n",
      "| Shashi|       3000|\n",
      "|   Ajay|      10000|\n",
      "|   Teja|      20000|\n",
      "|  Rahul|      10000|\n",
      "| Joseph|       4000|\n",
      "|Akshith|      10000|\n",
      "|   Abhi|       5000|\n",
      "| Mahesh|       4000|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GroupBy\n",
    "# Grouped to Find the Maximum Salary\n",
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc122ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   Name|avg(Salary)|\n",
      "+-------+-----------+\n",
      "|Karthik|     5000.0|\n",
      "|   Ravi|     2000.0|\n",
      "| Shashi|     3000.0|\n",
      "|   Ajay|    10000.0|\n",
      "|   Teja|    20000.0|\n",
      "|  Rahul|    10000.0|\n",
      "| Joseph|     4000.0|\n",
      "|Akshith|    10000.0|\n",
      "|   Abhi|     5000.0|\n",
      "| Mahesh|     4000.0|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "151d2264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  Department|sum(Salary)|\n",
      "+------------+-----------+\n",
      "|         NLP|      22000|\n",
      "|          AI|      18000|\n",
      "|          ML|      10000|\n",
      "|Data Science|      23000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby Department which gives Maximum Salary\n",
    "df_pyspark.groupBy('Department').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66fe5552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|  Department|      avg(Salary)|\n",
      "+------------+-----------------+\n",
      "|         NLP|          11000.0|\n",
      "|          AI|           6000.0|\n",
      "|          ML|           5000.0|\n",
      "|Data Science|7666.666666666667|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Department').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7bf192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|  Department|count|\n",
      "+------------+-----+\n",
      "|         NLP|    2|\n",
      "|          AI|    3|\n",
      "|          ML|    2|\n",
      "|Data Science|    3|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37b26cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21f03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
